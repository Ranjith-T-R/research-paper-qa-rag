# Research Paper Question Answering Using Retrievalâ€‘Augmented Generation (RAG)

 AI Research Assistant using RAG:

A Streamlitâ€‘powered Retrievalâ€‘Augmented Generation (RAG) assistant that lets you upload PDFs, build a FAISS vector index over text chunks (via HuggingFace embeddings), and query them using a local LLaMAÂ 2 model served through Ollama.

---

## ğŸš€ Features

- **PDF Ingestion**  
  Upload research papers (PDF) directly through Streamlit.  
- **Chunking & Embeddings**  
  Extract text with `PyPDFLoader`, split into overlapping chunks via `CharacterTextSplitter`, and embed using `sentence-transformers/all-MiniLM-L6-v2`.  
- **Vector Indexing**  
  Build/search a FAISS index for fast nearestâ€‘neighbor lookup. Saved locally for instant reloads.  
- **Local LLM QA**  
  Query your documents with a local LLaMAÂ 2 model (`ollama`) for fast, private inference.  
- **Sessionâ€‘state Caching**  
  Avoids redundant re-indexing or re-loading on every UI interaction.

---
## ğŸ“¦ Installation

1. **Clone this repo**  
   ```bash
   git clone https://github.com/<yourâ€‘username>/ai-research-assistant-rag.git
   cd ai-research-assistant-rag
2. **Create & activate a virtual environment (recommended)**
    ```bash
   python3 -m venv .venv
   source .venv/bin/activate
 
3. **Install dependencies**
    ```bash
    pip install -r requirements.txt
4. **Install & run Ollama**
   Follow Ollamaâ€™s quickstart to install.
   Then pull/run LLaMAÂ 2 locally:
    ```bash
   ollama pull llama2
   ollama run llama2
 ---
## âš™ï¸ Usage

1. **Launch the Streamlit app**  
   ```bash
   streamlit run app.py
2. **Upload a PDF**
In the sidebar, click Upload Research Paper and choose any .pdf.

3. **Wait for vector DB**
The app will split, embed, and index your document (takes ~10â€“30â€¯s).

4. **Ask questions**
Type any query in the input box and hit Enter.
â€“ It retrieves the top 3 relevant chunks, sends them to LLaMAÂ 2, and displays the answer.
---
## ğŸ›  Troubleshooting

1. **â€œmeta tensorâ€ error**
Ensure embeddings run on CPU by passing model_kwargs={"device": "cpu"}.

2. **Ollama connection issues**
Make sure ollama run llama2 is active before querying.

3. **Reâ€‘upload looping**
Add a simple sessionâ€‘state guard around st.experimental_rerun()
---
 
## ğŸ“ Project Structure

```bash
â”œâ”€â”€ app.py               # Streamlit entrypoint
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ docs/                # Uploaded PDFs
â”‚   â””â”€â”€ example.pdf      # Sample research paper
â”œâ”€â”€ vector_store_db/     # FAISS index folders
â”‚   â””â”€â”€ example/         # Index for example.pdf
â””â”€â”€ .env                 # Optional environment variables
---
